<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Road I travelled]]></title>
  <link href="http://MagicJun.github.io/atom.xml" rel="self"/>
  <link href="http://MagicJun.github.io/"/>
  <updated>2014-08-16T18:48:52+08:00</updated>
  <id>http://MagicJun.github.io/</id>
  <author>
    <name><![CDATA[JUN DUAN]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[文本分类]]></title>
    <link href="http://MagicJun.github.io/blog/2014/08/16/wen-ben-fen-lei/"/>
    <updated>2014-08-16T16:09:04+08:00</updated>
    <id>http://MagicJun.github.io/blog/2014/08/16/wen-ben-fen-lei</id>
    <content type="html"><![CDATA[<p>最近个人项目需要用到自动文本分类技术，这并不是什么研究课题，并不忌讳抄袭，相反能复用的轮子越多越好。需求是将博客（主要是中文）文章自动归类，方便给读者推荐阅读。思路是这样：
1. 中文分词</p>

<pre><code>中文分词大概是所有（中文）文本挖掘技术的基础，分词算法有很多类，字符串匹配+词典，统计模型等。互联网上有多种成熟的解决方案，我主要研究了下SAE的中文分词服务以及中科院的ictclas。SAE中文分词服务免费，但必须有appid，每次分词的文本大小不超过10KB，分词结果带有非常详细的词性标注，既然是服务，就无所谓语言支持了。ICTCLAS是个开源项目，支持C++，java等，考虑到ICTCLAS可能偏学术一些，我选用了SAE的中文分词，申请了一个appid，然后用php包装了一层，滤掉英文摘要，文本分段等等，向外提供rest接口，返回格式json。值的注意的是，这两种方法貌似对新词支持不够好，特别是ICTCLAS，“知乎”都分不出。
</code></pre>

<ol>
<li>计算分词TF-IDF值
 TF-IDF本来是用于关键词提取的，用以衡量每个term对文本的重要程度，TF就是词频，本文中该词出现越多说明其越重要，IDF是所谓的逆向文件概率，如果某词在很多文件中都出现过，则恰恰说明该词对该文本不重要，一个极端的例子就是停用词，“的”，“你我他”这种词是不能做关键词的。本文并没有搜集海量文本算IDF值，根据google出来的条目数IDF值，有个插曲是墙内google经常挂，为了完成这10k左右的词的google热度的收集，我想到了aws的机器，用php爬了好几天。
 本文计算TF = count(word) / count(*) ，IDF = log( (g<em>的</em>num+1) / g_word_num )；</li>
<li>特征词提取
 首先说一下训练集，互联网上有复旦大学的测试集和训练集可以下载，各有差不多9000篇文章，覆盖近20个类别（这已经算分得很细了）。文章内容以期刊、论文、出版物等为主，比较正式。这些文本不是utf-8格式的，在linux下使用必须批量转换过来。
 我们现在可以得到所有训练集中包含的词，那么应该选取哪些词作为特征来确定某文是属于某类呢？这就是特征提取的范畴。本文对每个词在所有文章中的TF-IDF summarize，然后取top 300个词做为特征词。根据这些特征词，就可以得到每篇文章的特征向量了，然后做归一化处理，基本训练的准备工作就做好了。
 这一步包含了很多文本处理工作，主要用perl完成。</li>
<li><p>训练分类器
 分类算法当然是最重要的啦，但是也是有一大堆轮子可以使用的，Weka里面集成了大量机器学习算法，回归，分类，聚类等，本文使用了其中的KNN文本分类算法。KNN就是说如果x的K个邻居都属于某类，那么x也被归为该类。weka很容易使用，我用的java，直接装个jar包就可以用了。weka的输入是ARFF文件，该文件的格式如下
 @relation traindata
 @attribute class (历史，文学&hellip;)
 @attribute 鲁迅 numetric
 @attribute &hellip;特征词&hellip; numeric</p>

<p> @data  把所有的9000多篇训练文章都放进来
 历史,0.234,0&hellip;. 每篇文章的特征向量</p>

<p> load完这个文件就可以训练了，classifier.buildClassifier(trainInstances); 训练完成会得到一个分类模型，这个模型可以序列化为文件，模型的好坏需要看测试结果，可以用classifier.classifyInstance(testInstance)对测试集进行检验，并且调整各个参数提高准确率;</p></li>
<li>使用分类器
 当得到一个准确率高的分类模型后，就可以投入使用了，load 一下模型，分析一下特征词，就可以根据这个模型算出类别了。</li>
</ol>


<p>总结：1-4步每步都能影响分类准确率，只能自己边实验边提高了。英文文本分类是有大公司提供api的，如yahoo的YQL。</p>

<p>References:
SAE中文分词服务：<a href="http://sae.sina.com.cn/doc/python/segment.html">http://sae.sina.com.cn/doc/python/segment.html</a>
ICTCLAS：<a href="http://ictclas.org/index.html">http://ictclas.org/index.html</a>
weka: <a href="http://www.cs.waikato.ac.nz/ml/weka/">http://www.cs.waikato.ac.nz/ml/weka/</a>
YQL: <a href="https://developer.yahoo.com/yql/">https://developer.yahoo.com/yql/</a></p>
]]></content>
  </entry>
  
</feed>
