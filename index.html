
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Road I travelled</title>
  <meta name="author" content="JUN DUAN">

  
  <meta name="description" content="场景1，知道明确的一组延时数据（不多），需要分析延时的分布；想清楚算法（排序），几行shell脚本应该可以完成。
场景2，这些延时数据是记录在apache的log文件中，怎么计算它们的分布呢？那么对应的数据处理会是这个样子的：首先需要找到延时数据的pattern（正则匹配， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://MagicJun.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Road I travelled" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Road I travelled</a></h1>
  
    <h2>from Beginners+ to Proficient, from worker to thinker</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:MagicJun.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/19/shu-ju-de-si-suo/">数据的思索</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-19T20:30:00+08:00" pubdate data-updated="true">Aug 19<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>场景1，知道明确的一组延时数据（不多），需要分析延时的分布；想清楚算法（排序），几行shell脚本应该可以完成。
场景2，这些延时数据是记录在apache的log文件中，怎么计算它们的分布呢？那么对应的数据处理会是这个样子的：首先需要找到延时数据的pattern（正则匹配，可能还需要处理log中各种奇葩字符），然后收集起来，一台机器一个文件，然后把所有的文件合成一个大文件。同样的几行shell脚本应该也可以完成。问题是：扫log毕竟是很慢的，并且log一般都会压缩，解压出来又慢又占用资源；最后生成的大文件可能相当大，超过200m sort估计就处理不了了，当然也可以用merge的方法来排序；
场景3，我们得想出办法存储这每天200m的数据，mysql不是一个可取的选择，mysql读写速度慢，表记录数超过千万后，查询速度更慢。分布式的KV数据库可以满足要求，key就是请求id，value是延时，并且设置过期时间。
场景4，如果用户又增加了几个数量级，并且需要记录的用户数据更多，每台机器平均QPS一千，100台机器，一天的数据量将是86亿条，86G，这就需要大数据工具了，apache将请求emit出来，通过数据高速通道到达hadoop集群，数据使用HBase（或者BigTable）来存，定义好数据的schema，然后使用map-reduce对海量数据进行分析。如果不精通map和reduce的写法，可以使用更高层的脚本，如pig等，实质都是一样的，都会拆成若干map的task和reduce task，然后分配到mapper和reducer上执行。如果需要每天都分析这个数据，就可以用oozie这种工作流工具了，通过定制自己的coordinator和workflow，把需要干的活及其先后顺序定义清楚，oozie就可以按时按需启动你的任务了。</p>

<p>果然这个时间写文章写不清楚，下次注意吧。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/17/guo-nei-yi-dong-hu-lian-wang-yan-gao-shi-chang-xian-zhuang/">国内移动互联网广告市场现状</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-17T17:00:18+08:00" pubdate data-updated="true">Aug 17<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>作为一个曾经的移动互联网从业者以及一枚互联网显示广告小兵，我树立了在移动互联网广告领域深耕的志向。</p>

<h2>移动互联网</h2>

<p>最早接触移动互联网是在研二，2010年，应该还算早吧。作为一名业余APP开发者，为这个市场贡献了上千万的流量（也包括垃圾流量），靠卖这点流量赚了几万块钱，也算小有成就。我没打算说运营商、牌照等移动互联网基础设施，我没有相关背景(就感觉移动网络越来越快，以后各种视频应用会应声上位)，就仅以一名勤奋（前后开发了60个小android app）的开发者身份说说这个市场。首先说一下移动互联网“两大”阵营（ios平台和android平台），不同的阵营其上的产业链也不同。ios有封闭的市场，统一的设备型号，以及比较严格的安全控制；android开源，google作为一条主线不断的进化os的代码，同时也有多家厂商fork了google的代码，创造了很多参差不齐的分支，开放的市场，开放的技术，多样的设备（从1元-5999元不等), 很难说最后谁统治谁，也有可能大家就这样你6我4的平衡着，就像现在的浏览器，每个人都有吃的，有多有少而已。然后说一下app，两个平台上的app数量旗鼓相当，可能现在android上的会更多；但我保证ios上app的平均质量要高于android上的，可能是因为adnroid开发门槛略低吧。我觉得质量好的App大抵是这样的：功能简洁明了；界面元素整齐划一；内容胜于形式；精于细节。最后一点大多数app都是做不到的，还有很多做到了前两点就草草上线了。随着各大互联网公司正在移动app上大展身手，势必会提高整个行业app的平均质量，也提升大众的审美眼光，以后那些用着不顺手不顺心的挫应用应该会很难生存了。</p>

<h2>传统互联网显示广告</h2>

<ul>
<li>工作之后有幸从事了复杂度堪比搜索的计算广告行业，了解了DSP,SSP,DMP,Exchange,CPM,CPC,ECPM等行业基础，也从事大型广告系统的开发与维护，知道了广告是怎么卖出去的，怎么让广告主做的广告有效果，怎么不毁网站主fancy的页面而让其获益，大家做买卖怎么算钱等看似简单的问题。做这一行是很需要技术的，因此做的好的都是互联网巨头，这些巨头都有大量优质的广告栏位可以卖，并且他们首先得保证自己的页面卖个好价钱，另外许多没有技术的广告主或是中小网站主就会onboard到他们的系统中做买卖，巨头们会收取相应技术费用。每个巨头都有一套或多套广告系统，互相之间各种卖来卖去。</li>
<li>技术难度在于计算规模大，平均每次展示都要上万次的计算，而巨头们平均每天的展示都是billion级别的。这里面有很多性能调优、海量数据挖掘、实时竞价算法等有意思的问题。我觉得我能够在毕业之初就涉足这一领域是人生一大幸事。虽然这个领域已经相当成熟，但是对于我来说还是有太多太多可以学的。</li>
</ul>


<h2>移动广告在中国</h2>

<ul>
<li>作为一名android开发者，我接触了应用市场，移动广告平台（提供sdk的），广告聚合平台，数据分析商等几种实体。</li>
<li>应用市场的早期使命是收录尽可能多的app，google marketplace里面有的我一定要有，然后卖首页，卖排名，卖少量广告,预装等。4年前我接触过的这些市场现在基本都还在，安智市场，安卓市场，木蚂蚁市场，豌豆荚市场，N多市场等等应该不下10家。对于应用市场而言，现在不是揽量了，重心倾斜到了追求品质，安智最近不能上架有广告的应用（？可能中国用户的身心已经被“卸载应用也能显示广告”的应用给伤到了），这一点我自我反省一下，很多开发者，广告平台太急功近利，把用户体验不当回事，放的广告具有丑，烦，三俗等特点, 坑用户就是坑自己。</li>
<li>移动广告平台也有不下20家，4年了，好像我用过的现在都还活着：多盟，adwo,忆动智道，百分联通，有米等，那时候还没有百度移动联盟，但是感觉现在有被其一统江湖的趋势，这些广告平台大抵做这么一件事情，劝唆尽可能多的app装我的sdk(你来我这儿卖，单价能到1元，填充99。9%), 然后onboard尽可能多的财主（你来我这儿打广告，我这儿有各种形式的广告，插屏，开屏，banner，支持多种计费方式，保证你花最少的钱干最多的事。），但我觉得这些人大多只是做了一个壳，这个壳就是搭建了一个简单的广告交易平台，
开发者卖了一个流量过来，他就在那儿挑一挑，这个广告商今天的钱还没花完，帮他花掉，然后把他的物料传给开发者。如果开发者问，为什么我这么牛逼的app卖的价钱跟垃圾app一样都是两毛，为什么我的点击率这么低？如果广告商问，你这儿的ROI怎么这么低啊，有些流量你能不能别花那么多钱啊？这些广告平台大抵是答不上来的。</li>
<li>广告聚合平台不多，我知道的就是芒果聚合以及Adview，聚合平台做的事是这样的：开发者你好，我就是为你服务的，你用我的sdk，我能帮你管理各种广告平台，给他们设置投放比例，帮你分析用户，展示信息。这种公司没有钱流进来，因此不赚钱的，但是赚了大量的数据，包括开发者的以及广告商的，并且沉得下来研究数据，我觉得有勇气做这个行业的都是准备做点事的，芒果现在主推的“程序化购买”证明了这一点，能不能成事就看实践的怎样了。</li>
<li>最后一种是数据分析商，友盟，创新工场孵化的公司，我没有用过，因为我没有一款产品需要专业的数据分析。在传统行业中有一种为广告主提供大量用户数据分析的实体DMP,
但是友盟是为开发者提供服务的，像是SSP，他现在拥有的数据已经够他卖个好价钱了（要是数据能卖的话），因为他能统计出每个app的用户是博士还是硕士，是从事医疗还是销售。

<h2>方向</h2>

移动DSP吧，传统行业的DSP已经在进军这一块了，并且3中说的很多广告平台正意识到自己外强中干的属性了，准备边学边做程序化购买，做好一个移动DSP。所以这一块是当前最缺的，也会是以后竞争最激烈的。

<h2>准备</h2>

<p>自己实现一个移动DSP的雏形吧，做是最好的学，然后每次有新想法都及时记录下来。</p></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/16/wen-ben-fen-lei/">文本分类</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-16T16:09:04+08:00" pubdate data-updated="true">Aug 16<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>最近个人项目需要用到自动文本分类技术，这并不是什么研究课题，并不忌讳抄袭，相反能复用的轮子越多越好。需求是将博客（主要是中文）文章自动归类，方便为读者提供个性化推荐。思路是这样：</br>
1. 中文分词
</br>中文分词大概是所有（中文）文本挖掘技术的基础，分词算法有很多类，字符串匹配+词典，统计模型等。互联网上有多种成熟的解决方案，我主要研究了下SAE的中文分词服务以及中科院的ictclas。SAE中文分词服务免费，但必须有appid，每次分词的文本大小不超过10KB，分词结果带有非常详细的词性标注，既然是服务，就无所谓语言支持了。ICTCLAS是个开源项目，支持C++，java等，考虑到ICTCLAS可能偏学术一些，我选用了SAE的中文分词，申请了一个appid，然后用php包装了一层，滤掉英文摘要，文本分段等等，向外提供rest接口，返回格式json。值的注意的是，这两种方法貌似对新词支持不够好，特别是ICTCLAS，“知乎”都分不出。
2. 计算分词TF-IDF值
</br>TF-IDF本来是用于关键词提取的，用以衡量每个term对文本的重要程度，TF就是词频，本文中该词出现越多说明其越重要，IDF是所谓的逆向文件概率，如果某词在很多文件中都出现过，则恰恰说明该词对该文本不重要，一个极端的例子就是停用词，“的”，“你我他”这种词是不能做关键词的。本文并没有搜集海量文本算IDF值，根据google出来的条目数IDF值，有个插曲是墙内google经常挂，为了完成这10k左右的词的google热度的收集，我想到了aws的机器，用php爬了好几天。</p>

<pre><code>本文计算TF = count(word) / count(*) ，IDF = log( (g_的_num+1) / g_word_num )；
</code></pre>

<ol>
<li>特征词提取
</br>首先说一下训练集，互联网上有复旦大学的测试集和训练集可以下载，各有差不多9000篇文章，覆盖近20个类别（这已经算分得很细了）。文章内容以期刊、论文、出版物等为主，比较正式。这些文本不是utf-8格式的，在linux下使用必须批量转换过来。
</br>我们现在可以得到所有训练集中包含的词，那么应该选取哪些词作为特征来确定某文是属于某类呢？这就是特征提取的范畴。本文对每个词在所有文章中的TF-IDF summarize，然后取top 300个词做为特征词。根据这些特征词，就可以得到每篇文章的特征向量了，然后做归一化处理，基本训练的准备工作就做好了。
</br>这一步包含了很多文本处理工作，主要用perl完成。</li>
<li><p>训练分类器
</br>分类算法当然是最重要的啦，但是也是有一大堆轮子可以使用的，Weka里面集成了大量机器学习算法，回归，分类，聚类等，本文使用了其中的KNN文本分类算法。KNN就是说如果x的K个邻居都属于某类，那么x也被归为该类。weka很容易使用，我用的java，直接装个jar包就可以用了。weka的输入是ARFF文件，该文件的格式如下:
 </br>@relation traindata
 </br>@attribute class (历史，文学&hellip;)
 </br>@attribute 鲁迅 numetric
 </br>@attribute &hellip;特征词&hellip; numeric
 </br>@data  把所有的9000多篇训练文章都放进来
 </br>历史,0.234,0&hellip;. 每篇文章的特征向量</p>

<p> load完这个文件就可以训练了，classifier.buildClassifier(trainInstances); 训练完成会得到一个分类模型，这个模型可以序列化为文件，模型的好坏需要看测试结果，可以用classifier.classifyInstance(testInstance)对测试集进行检验，并且调整各个参数提高准确率;</p></li>
<li>使用分类器
 当得到一个准确率高的分类模型后，就可以投入使用了，load 一下模型，分析一下特征词，就可以根据这个模型算出类别了。</li>
</ol>


<p>总结：1-4步每步都能影响分类准确率，只能自己边实验边提高了。英文文本分类是有大公司提供api的，如yahoo的YQL。</p>

<p>References:</br>
SAE中文分词服务：<a href="http://sae.sina.com.cn/doc/python/segment.html">http://sae.sina.com.cn/doc/python/segment.html</a></br>
ICTCLAS：<a href="http://ictclas.org/index.html">http://ictclas.org/index.html</a></br>
weka: <a href="http://www.cs.waikato.ac.nz/ml/weka/">http://www.cs.waikato.ac.nz/ml/weka/</a></br>
YQL: <a href="https://developer.yahoo.com/yql/">https://developer.yahoo.com/yql/</a></br></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/08/19/shu-ju-de-si-suo/">数据的思索</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/17/guo-nei-yi-dong-hu-lian-wang-yan-gao-shi-chang-xian-zhuang/">国内移动互联网广告市场现状</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/08/16/wen-ben-fen-lei/">文本分类</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - JUN DUAN -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
